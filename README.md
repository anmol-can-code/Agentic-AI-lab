# Fine_tuning_Lab_task1.ipynb

This notebook demonstrates fine-tuning of a Small Language Model (DistilGPT-2)
on a subset of the OpenWebText dataset using Google Colab.

## Key Details
- Model: DistilGPT-2 (< 3B parameters)
- Dataset: OpenWebText (subset)
- Task: Causal Language Modeling
- Evaluation Metric: Perplexity (~30)

## Tools Used
- Hugging Face Transformers
- Datasets library
- Google Colab (T4 GPU)
